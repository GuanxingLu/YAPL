# Vision-language-action Models

* **arxiv:2406** : OpenVLA: An Open-Source Vision-Language-Action Model
* **ECCV'22** : Extract free dense labels from clip
> Used by GeFF to distill vision-language feature

* **arxiv'2407**: Robotic Control via Embodied Chain-of-Thought Reasoning
> VLA Chain-of-thought

* **arxiv:2405**: ManiFoundation Model for General-Purpose Robotic Manipulation of
Contact Synthesis with Arbitrary Objects and Robots

* **arxiv:2405**: Neural Scaling Laws for Embodied AI

## Vision-language Navigation

* **arxiv:2407** : Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models

* **arxiv:2407**: Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs
> VLA -> Goal image -> ImageNav

* **CoRL'23** SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning

## Vision-language Manipulation
* **[RSS'24]** Manipulate-Anything: Automating Real-World Robots using Vision-Language Models 
> 1. VLM identifies objects and determines sub-goals. 2. action generation module. 3. sub-goal verification module for error recovery. Filter the trajectories to obtain successful demonstrations for downstream policy training. 

## Large-scale Robotic Datasets

* DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset
* RH20T
* RoboSet
* BridgeData V2
* DobbE
* Open X-Embodiment

No language instruction:
* MIME
* RoboTurk
* RoboNet
* MT-Ope
